# **Crawler**

**Crawler** - в простонародии - _краулер_ - небольшая утилита для обхода страниц, начиная с заданной и загрузки файлов оттуда.

## Требования
* Python 3.10.x или более новая версия
* requests~=2.32.3
* aiohttp~=3.9.5
* beautifulsoup4~=4.12.3
* asyncclick~=8.1.7.2
* bs4~=0.0.2

Установить необходимые зависимости можно с помощью requirements.txt:

    pip install -r requirements.txt

## Основные функции:

Для запуска Вам нужно ввести в терминале/командной строке:
```
путь к файлу/python cli.py
```
, а также указать необходимые аргументы:

  * --site TEXT        URL сайта для сканирования или файл с URL'ами в формате .txt.  [required]
  * --depth INTEGER    Глубина сканирования (по умолчанию 3).
  * --path TEXT        Папка, куда скачивать сайты (по умолчанию - эта)
  * --maxsize INTEGER  Максимальный размер файлов для скачивания в КБ (по
                       умолчанию - 1024)
  * --rtypes TEXT      Какие типы файлов сохранять (по умолчанию - все)
  * --ntypes TEXT      Какие типы файлов не сохранять  (по умолчанию - никакие)
  * --nurls TEXT       Какие домены игнорировать при обходе (по умолчанию -
                       никакие) Например: --nurls admin позволит не обходить
                       сайты с admin в названии
  * --bots INTEGER     Количество ботов для обхода (по умолчанию - 4)
  * --help             Помощь


После этого начнётся загрузка всего содержимого (с учётом ограничений) в указанную папку.
Также будет проведено разделение по папкам, соответствующее различным сайтам.

### Robots.txt
Краулер поддерживает robots.txt, т.е., например, если какой-то сайт будет против использования его ресурсов, то они скачиваться не будут.

